import time
from bs4 import BeautifulSoup
from .base_crawler import BaseCrawler
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys

from typing import Dict

class WantedCrawler(BaseCrawler):
    # ì›í‹°ë“œ ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬

    def __init__(self):
        # ë¶€ëª¨ í´ë˜ìŠ¤ __init__ì„ í˜¸ì¶œí•˜ì—¬ base_url ì „ë‹¬
        super().__init__("https://www.wanted.co.kr/")

    def crawl(self, keyword: str = "ë°±ì—”ë“œ", pages_to_crawl: int = 1, is_newbie: bool = False):
        # BaseCrawlerì˜ ì˜ë¬´ ì¡°í•­ì„ ì‹¤ì œë¡œ êµ¬í˜„, ì›í‹°ë“œ ì±„ìš© ì •ë³´ í¬ë¡¤ë§í•˜ì—¬ list of dict í˜•íƒœë¡œ ë°˜í™˜
        print(f"ì›í‹°ë“œì—ì„œ '{keyword}' í‚¤ì›Œë“œë¡œ í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        # í¬ì§€ì…˜ íƒ­ì˜ URLë¡œ ë°”ë¡œ ì ‘ê·¼í•˜ì—¬ ë¶ˆí•„ìš”í•œ í´ë¦­ ê³¼ì • ìƒëµ
        target_url = f"{self.base_url}/search?query={keyword}&tab=position"
        self.driver.get(target_url)
        self._random_sleep()

        # ë¬´í•œ ìŠ¤í¬ë¡¤ë¡œ ë°ì´í„° ìˆ˜ì§‘
        print(" - ë¬´í•œ ìŠ¤í¬ë¡¤ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        last_height = self.driver.execute_script('return document.documentElement.scrollHeight')

        # í‚¤ë³´ë“œ ì…ë ¥ì„ ë°›ì„ ëŒ€ìƒì„ ì§€ì •
        body = self.driver.find_element(By.TAG_NAME, 'body')

        patience = 5
        patience_counter = 0

        while True:
            body.send_keys(Keys.END)


            # self.driver.execute_script('window.scrollTo(0, document.documentElement.scrollHeight);')
            self._random_sleep()
            new_height = self.driver.execute_script('return document.documentElement.scrollHeight')

            if new_height == last_height:
                patience_counter += 1
                print(f" - í˜ì´ì§€ ë†’ì´ ë³€í™” ì—†ìŒ.(ì¸ë‚´ì‹¬: {patience_counter}/{patience})")
                if patience_counter >= patience:
                    print(" - ìŠ¤í¬ë¡¤ì´ í˜ì´ì§€ ëì— ë„ë‹¬í–ˆê±°ë‚˜, ë” ì´ìƒ ë¡œë”©ë˜ì§€ ì•Šì•„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                    break
            else:
                # ë†’ì´ê°€ ë³€í–ˆë‹¤ë©´, ì¸ë‚´ì‹¬ ì¹´ìš´í„° ì´ˆê¸°í™”
                patience_counter = 0

            last_height = new_height

        # ë°ì´í„° ì¶”ì¶œ ë° ê°€ê³µ
        job_data = []
        soup = BeautifulSoup(self.driver.page_source, 'lxml')
        job_cards = soup.select("div[role='listitem'] a")

        for card in job_cards:
            try:
                title = card.select_one("strong[class*='JobCard_title']").text
                company_name = card.select_one("span[class*='CompanyName']").text
                link = "https://www.wanted.co.kr" + card['href']
                job_data.append({"title": title, "company": company_name, "link":link, "source": "Wanted"})
            except Exception:
                continue
        print(f"ì›í‹°ë“œì—ì„œ ì´ {len(job_data)}ê°œì˜ ê³µê³ ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        return job_data
    
    def get_job_description(self, url: str) -> Dict[str, str]:
        # ì›í‹°ë“œì˜ ìƒì„¸ í˜ì´ì§€ì˜ ë³¸ë¬¸ ë‚´ìš© ìˆ˜ì§‘:ìƒì„¸í˜ì´ì§€ì— ë°©ë¬¸, 'ë”ë³´ê¸°'ë²„íŠ¼ í´ë¦­, ì „ì²´ ë³¸ë¬¸ ë‚´ìš© ìˆ˜ì§‘
        description = ""
        deadline = "í™•ì¸ í•„ìš”"

        try:
            self.driver.get(url)
            self._random_sleep()

            # 1. í˜ì´ì§€ í¼ì¹˜ëŠ” ë²„íŠ¼ í´ë¦­ ë¡œì§
            try:
                more_button_xpath = "//button[span[text()='ìƒì„¸ ì •ë³´ ë” ë³´ê¸°']]"
                more_button = self.driver.find_element(By.XPATH, more_button_xpath)

                self.driver.execute_script("arguments[0].click();", more_button)
                print("   -> 'ìƒì„¸ ì •ë³´ ë” ë³´ê¸°' ë²„íŠ¼ì„ í´ë¦­í–ˆìŠµë‹ˆë‹¤.")
                self._random_sleep()
            except Exception:
                print("   -> 'ìƒì„¸ ì •ë³´ ë” ë³´ê¸°' ë²„íŠ¼ì´ ì—†ê±°ë‚˜ í´ë¦­í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê·¸ëŒ€ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                pass


            soup = BeautifulSoup(self.driver.page_source, 'lxml')

            # 2. ì „ì²´ ë³¸ë¬¸ì„ ê°ì‹¸ëŠ” article íƒœê·¸ ì°¾ê¸°
            content_article = soup.select_one('article[class*="JobDescription_JobDescription"]')

            if content_article:
                description = content_article.text.strip()
            else:
                print("   -> [ê²½ê³ ] ê¸°ë³¸ ì„ íƒìë¡œ ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. 2ì°¨ ì„ íƒìë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")

                # h2 íƒœê·¸ ì¤‘ 'í¬ì§€ì…˜ ìƒì„¸'ë¼ëŠ” í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ ìš”ì†Œì˜ ë¶€ëª¨ë¥¼ ì°¾ëŠ”ë‹¤
                h2 = soup.find('h2', string='í¬ì§€ì…˜ ìƒì„¸')
                if h2 and h2.parent:
                    description = h2.parent.text.strip()
                
        except Exception as e:
            print(f" ğŸš¨ [ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì˜¤ë¥˜] {url} ì²˜ë¦¬ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")
        
        return {'description': description, 'deadline':deadline}
 