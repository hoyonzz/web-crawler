import os
import json
from datetime import datetime
from dotenv import load_dotenv
import notion_client


from crawlers.wanted_crawler import WantedCrawler
from crawlers.jobkorea_crawler import JobKoreaCrawler
from crawlers.saramin_crawler import SaraminCrawler

from data_processor.personalized_job_filter import PersonalizedJobFilter
from analysis.gemini_analyzer import analyze_job_posting_with_ai


load_dotenv()

# ν…μ¤νΈ μ‹ λ¶„μ„ν•  μµλ€ κ°―μ
TEST_MODE_LIMIT = 3

# μ„¤μ • μ΄κΈ°ν™”
NOTION_API_KEY = os.environ.get("NOTION_API_KEY")
NOTION_DATABASE_ID = os.environ.get("NOTION_DATABASE_ID")
notion = notion_client.Client(auth=NOTION_API_KEY)


# νμ΄ν”„λΌμΈ μ‹μ‘ 
print("π€ [1λ‹¨κ³„] λ¨λ“  μ‚¬μ΄νΈμ—μ„ κΈ°λ³Έ κ³µκ³  λ©λ΅ μμ§‘μ„ μ‹μ‘ν•©λ‹λ‹¤.")
all_jobs_basic_info = []
# ν΄λμ¤ μμ²΄λ¥Ό λ¦¬μ¤νΈμ— λ‹΄μ•„, ν•„μ”ν•  λ• κ°μ²΄λ¥Ό μƒμ„±ν•λ” λ°©μ‹μΌλ΅ λ³€κ²½
crawlers_to_run = [WantedCrawler, JobKoreaCrawler, SaraminCrawler]

for crawlerClass in crawlers_to_run:
    crawler = crawlerClass()
    crawler_name = type(crawler).__name__
    print(f"\n--- {crawler_name} μ‹¤ν–‰ ---")
    try:
        jobs = crawler.crawl(keyword='λ°±μ—”λ“', pages_to_crawl=1, is_newbie=True)
        all_jobs_basic_info.extend(jobs)
        print(f"   -> {len(jobs)}κ°μ κ³µκ³  λ©λ΅ μμ§‘ μ™„λ£.")
    except Exception as e:
        print(f" π¨ [μ¤λ¥] {crawler_name} λ©λ΅ μμ§‘ μ‹¤ν¨: {e} ")
    finally:
        crawler.close_driver()
print(f"\nβ… [1λ‹¨κ³„ μ™„λ£] μ΄ {len(all_jobs_basic_info)}κ°μ κ³µκ³  λ©λ΅ μμ§‘ μ™„λ£.")

# μ¤‘λ³µ μ κ±° λ‹¨κ³„
print(f"\nπ€ [2λ‹¨κ³„] Notion DBμ™€ λΉ„κµν•μ—¬ μ‹ κ· κ³µκ³  ν•„ν„°λ§ μ‹μ‘...")
new_jobs_basic_info = []
for job in all_jobs_basic_info:
    link = job.get('link')
    if not link: continue
    try:
        response = notion.databases.query(
            database_id=NOTION_DATABASE_ID,
            filter={"property": "λ§ν¬", "url": {"equals": link}}
            )
        if len(response['results']) == 0:
            new_jobs_basic_info.append(job)
    except Exception as e:
        print(f"π¨ [μ¤λ¥] DB μ¤‘λ³µ ν™•μΈ μ‹¤ν¨: {job.get('title')}, {e}")
print(f"\nβ… [2λ‹¨κ³„ μ™„λ£] {len(new_jobs_basic_info)}κ°μ μƒλ΅μ΄ κ³µκ³  λ°κ²¬.")

# μƒμ„Έ μ •λ³΄ μμ§‘ λ‹¨κ³„
jobs_to_process = new_jobs_basic_info[:TEST_MODE_LIMIT] if TEST_MODE_LIMIT is not None else new_jobs_basic_info
print(f"\nπ€ [3λ‹¨κ³„] {len(jobs_to_process)}κ° μ‹ κ· κ³µκ³ μ μƒμ„Έ μ •λ³΄ μμ§‘ μ‹μ‘...")
full_new_jobs, failed_count = [], 0  
crawler_instances = {cls.__name__: cls() for cls in crawlers_to_run}

for job in jobs_to_process:
    source_crawler_name = job.get('source', '') + "Crawler"
    crawler = crawler_instances.get(source_crawler_name)
    if not crawler: continue

    try:
        details = crawler.get_job_description(job['link'])
        job.update(details)
        full_new_jobs.append(job)
    except Exception as e:
        print(f"π¨ [μ¤λ¥] μƒμ„Έ μ •λ³΄ μμ§‘ μ‹¤ν¨: {job.get('title')}, {e}")
        failed_count += 1

for crawler in crawler_instances.values():
    crawler.close_driver()

print(f"\nβ…[3λ‹¨κ³„ μ™„λ£] {len(full_new_jobs)}κ°μ κ³µκ³ μ— λ€ν• μƒμ„Έ μ •λ³΄ μμ§‘μ„ μ™„λ£ν–μµλ‹λ‹¤.")


# λ¶„μ„ λ° Notion μ €μ¥
print("\nπ€[4λ‹¨κ³„] κ°μΈν™” ν•„ν„°λ§, AI λ¶„μ„ λ° Notion μ €μ¥ μ‹μ‘...")
job_filter = PersonalizedJobFilter()
success_count = 0
filtered_count = 0

for i, job in enumerate(full_new_jobs):
    title = job.get('title', 'μ λ© μ—†μ')
    description = job.get('description', '')

    print(f"\n--- ({i+1} / {len(full_new_jobs)}) λ¶„μ„ μ¤‘: {title}---")

    # 3-1. κ°μΈν™” ν•„ν„°λ§
    is_relevant, score = job_filter.calculate_relevance_score(title, description)
    if not is_relevant:
        print(f" -> [ν•„ν„°λ§λ¨] κ΄€λ ¨λ„κ°€ λ‚®μ•„ κ±΄λ„λλ‹λ‹¤. (μ μ: {score})")
        filtered_count += 1
        continue

    print(f" -> [ν†µκ³Ό] κ΄€λ ¨λ„ λ†’μ (μ μ: {score}). AI λ¶„μ„μ„ μ‹μ‘ν•©λ‹λ‹¤...")

    # 3-2. Gemini AI λ¶„μ„
    matched_skills = job_filter.extract_matched_skills(description)
    ai_analysis_json = analyze_job_posting_with_ai(title, description, matched_skills)

    if not ai_analysis_json:
        print(" -> π¨[μ¤λ¥] AI λ¶„μ„μ— μ‹¤ν¨ν–μµλ‹λ‹¤. λ‹¤μ κ³µκ³ λ΅ λ„μ–΄κ°‘λ‹λ‹¤.")
        continue

    # 3-3. Notionμ— μ €μ¥
    ai_analysis_text = json.dumps(ai_analysis_json, ensure_ascii=False, indent = 2)
    properties_to_save = {
        'μ§λ¬΄':{'title':[{'text':{'content':title}}]},
        'νμ‚¬λ…':{'rich_text': [{'text':{'content':job.get('company', 'νμ‚¬λ… μ—†μ')}}]},
        'λ§ν¬':{'url':job.get('link')},
        'μ¶μ²':{'rich_text':[{'text':{'content':job.get('source', 'μ¶μ² μ—†μ')}}]},
        'μμ§‘μΌ':{'date':{'start':datetime.now().strftime("%Y-%m-%d")}},
        'λ§κ°μΌ':{'rich_text':[{'text':{'content':job.get('deadline', 'ν™•μΈ ν•„μ”')}}]},
        'AI λ¶„μ„ κ²°κ³Ό': {'rich_text':[{'text':{'content':ai_analysis_text}}]}
    }

    try:
        notion.pages.create(parent={"database_id":NOTION_DATABASE_ID}, properties=properties_to_save)
        success_count += 1
        print(f"    -> β… '{title}' Notion μ €μ¥ μ„±κ³µ!")
    except Exception as e:
        print(f"   π¨ [μ¤λ¥] '{title}' μ €μ¥ μ‹¤ν¨! μ›μΈ: {e}")


# μµμΆ… κ²°κ³Ό μ”μ•½
print("-" * 30)
print("β… λ¨λ“  νμ΄ν”„λΌμΈμ΄ μ™„λ£λμ—μµλ‹λ‹¤.")
print(f"  - μ΄ {success_count}κ°μ μƒλ΅μ΄ κ³µκ³ λ¥Ό Notionμ— μ €μ¥ν–μµλ‹λ‹¤.")
print(f"  - {failed_count}κ°μ κ³µκ³ λ” μƒμ„Έ μ •λ³΄ μμ§‘ μ¤‘ μ¤λ¥κ°€ λ°μƒν–μµλ‹λ‹¤.")
print(f"  - {filtered_count}κ°μ κ³µκ³ λ” ν•„ν„°λ§λμ–΄ μ μ™Έλμ—μµλ‹λ‹¤.")
print("-" * 30)