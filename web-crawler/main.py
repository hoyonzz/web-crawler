import os
import json
from datetime import datetime
from dotenv import load_dotenv
import notion_client
import time
import traceback


from crawlers.wanted_crawler import WantedCrawler
from crawlers.jobkorea_crawler import JobKoreaCrawler
from crawlers.saramin_crawler import SaraminCrawler

from data_processor.personalized_job_filter import PersonalizedJobFilter
from analysis.gemini_analyzer import analyze_job_posting_with_ai


load_dotenv()

# í…ŒìŠ¤íŠ¸ ì‹œ ë¶„ì„í•  ìµœëŒ€ ê°¯ìˆ˜
TEST_MODE_LIMIT = 5

# ì„¤ì • ì´ˆê¸°í™”
NOTION_API_KEY = os.environ.get("NOTION_API_KEY")
NOTION_DATABASE_ID = os.environ.get("NOTION_DATABASE_ID")
notion = notion_client.Client(auth=NOTION_API_KEY)


# íŒŒì´í”„ë¼ì¸ ì‹œì‘ 
print("ğŸš€ [1ë‹¨ê³„] ëª¨ë“  ì‚¬ì´íŠ¸ì—ì„œ ê¸°ë³¸ ê³µê³  ëª©ë¡ ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
all_jobs_basic_info = []
# í´ë˜ìŠ¤ ìì²´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„, í•„ìš”í•  ë•Œ ê°ì²´ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½
crawlers_to_run = [WantedCrawler, JobKoreaCrawler, SaraminCrawler]

for crawlerClass in crawlers_to_run:
    crawler = crawlerClass()
    crawler_name = type(crawler).__name__
    print(f"\n--- {crawler_name} ì‹¤í–‰ ---")
    try:
        pages = 5 if crawler_name != 'WantedCrawler' else 1
        jobs = crawler.crawl(keyword='ë°±ì—”ë“œ', pages_to_crawl=pages, is_newbie=True)
        all_jobs_basic_info.extend(jobs)
        print(f"   -> {len(jobs)}ê°œì˜ ê³µê³  ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ.")
    except Exception as e:
        print(f" ğŸš¨ [ì˜¤ë¥˜] {crawler_name} ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨: {e} ")
    finally:
        crawler.close_driver()
print(f"\nâœ… [1ë‹¨ê³„ ì™„ë£Œ] ì´ {len(all_jobs_basic_info)}ê°œì˜ ê³µê³  ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ.")

# ì¤‘ë³µ ì œê±° ë‹¨ê³„
print(f"\nğŸš€ [2ë‹¨ê³„] Notion DBì™€ ë¹„êµí•˜ì—¬ ì‹ ê·œ ê³µê³  í•„í„°ë§ ì‹œì‘...")
new_jobs_basic_info = []
for job in all_jobs_basic_info:
    link = job.get('link')
    if not link: continue
    try:
        response = notion.databases.query(
            database_id=NOTION_DATABASE_ID,
            filter={"property": "ë§í¬", "url": {"equals": link}}
            )
        if len(response['results']) == 0:
            new_jobs_basic_info.append(job)
    except Exception as e:
        print(f"ğŸš¨ [ì˜¤ë¥˜] DB ì¤‘ë³µ í™•ì¸ ì‹¤íŒ¨: {job.get('title')}, {e}")
print(f"\nâœ… [2ë‹¨ê³„ ì™„ë£Œ] {len(new_jobs_basic_info)}ê°œì˜ ìƒˆë¡œìš´ ê³µê³  ë°œê²¬.")

# ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ë‹¨ê³„
jobs_to_process = new_jobs_basic_info[:TEST_MODE_LIMIT] if TEST_MODE_LIMIT is not None else new_jobs_basic_info
print(f"\nğŸš€ [3ë‹¨ê³„] {len(jobs_to_process)}ê°œ ì‹ ê·œ ê³µê³ ì˜ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹œì‘...")
full_new_jobs, failed_count = [], 0  
crawler_instances = {cls.__name__: cls() for cls in crawlers_to_run}

for idx, job in enumerate(jobs_to_process, 1):
    source_crawler_name = job.get('source', '') + "Crawler"
    crawler = crawler_instances.get(source_crawler_name)
    if not crawler:
        print(f"   ({idx}/{len(jobs_to_process)}) âš ï¸ í¬ë¡¤ëŸ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {source_crawler_name}")
        failed_count += 1
        continue
    
    title = job.get('title', 'ì œëª© ì—†ìŒ')
    print(f"   ({idx}/{len(jobs_to_process)}) ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {title}")
    try:
        details = crawler.get_job_description(job['link'])
        if not details:
            print(f"   âš ï¸ ìƒì„¸ ì •ë³´ê°€ Noneì…ë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            failed_count += 1
            continue
        description = details.get('description', '')
        
        if not description or len(description.strip()) < 50:
            print(f"   âš ï¸ ê³µê³  ì„¤ëª…ì´ ë¹„ì—ˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ê¸¸ì´: {len(description)}ì). ê±´ë„ˆëœë‹ˆë‹¤.")
            failed_count += 1
            continue
            
        job.update(details)
        full_new_jobs.append(job)
        print(f"    âœ… ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì„±ê³µ (ì„¤ëª… ê¸¸ì´: {len(description)}ì)")
        
    except Exception as e:
        print(f"ğŸš¨ [ì˜¤ë¥˜] ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨: {job.get('title')}, {e}")
        traceback.print_exc()
        failed_count += 1

for crawler in crawler_instances.values():
    crawler.close_driver()

print(f"\nâœ…[3ë‹¨ê³„ ì™„ë£Œ] {len(full_new_jobs)}ê°œì˜ ê³µê³ ì— ëŒ€í•œ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ì„ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤.")


# ë¶„ì„ ë° Notion ì €ì¥
print("\nğŸš€[4ë‹¨ê³„] ê°œì¸í™” í•„í„°ë§, AI ë¶„ì„ ë° Notion ì €ì¥ ì‹œì‘...")
job_filter = PersonalizedJobFilter()
success_count = 0
filtered_count = 0

for i, job in enumerate(full_new_jobs):
    title = job.get('title', 'ì œëª© ì—†ìŒ')
    description = job.get('description', '')

    print(f"\n--- ({i+1} / {len(full_new_jobs)}) ë¶„ì„ ì¤‘: {title}---")

    # 3-1. ê°œì¸í™” í•„í„°ë§
    is_relevant, score = job_filter.calculate_relevance_score(title, description)
    if not is_relevant:
        print(f" -> [í•„í„°ë§ë¨] ê´€ë ¨ë„ê°€ ë‚®ì•„ ê±´ë„ˆëœë‹ˆë‹¤. (ì ìˆ˜: {score})")
        filtered_count += 1
        time.sleep(0.5)
        continue

    print(f" -> [í†µê³¼] ê´€ë ¨ë„ ë†’ìŒ (ì ìˆ˜: {score}). AI ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # 3-2. Gemini AI ë¶„ì„
    matched_skills = job_filter.extract_matched_skills(description)
    ai_analysis_json = analyze_job_posting_with_ai(title, description, matched_skills)

    if not ai_analysis_json:
        print(" -> ğŸš¨[ì˜¤ë¥˜] AI ë¶„ì„ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ê³µê³ ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
        # ë¶„ì„ ì‹¤íŒ¨ ì‹œì—ë„ API ê³¼ë¶€í•˜ë¥¼ í”¼í•˜ê¸° ìœ„í•´ ëŒ€ê¸°í•˜ê¸°
        print(" -> API ì†ë„ ì œì–´ë¥¼ ìœ„í•´ 5ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤...")
        time.sleep(5)
        continue

    # 3-3. Notionì— ì €ì¥
    ai_analysis_text = json.dumps(ai_analysis_json, ensure_ascii=False, indent = 2)
    properties_to_save = {
        'ì§ë¬´':{'title':[{'text':{'content':title}}]},
        'íšŒì‚¬ëª…':{'rich_text': [{'text':{'content':job.get('company', 'íšŒì‚¬ëª… ì—†ìŒ')}}]},
        'ë§í¬':{'url':job.get('link')},
        'ì¶œì²˜':{'rich_text':[{'text':{'content':job.get('source', 'ì¶œì²˜ ì—†ìŒ')}}]},
        'ìˆ˜ì§‘ì¼':{'date':{'start':datetime.now().strftime("%Y-%m-%d")}},
        'ë§ˆê°ì¼':{'rich_text':[{'text':{'content':job.get('deadline', 'í™•ì¸ í•„ìš”')}}]},
        'AI ë¶„ì„ ê²°ê³¼': {'rich_text':[{'text':{'content':ai_analysis_text}}]},
        'ê´€ë ¨ë„ ì ìˆ˜': {'number': score}
    }

    try:
        notion.pages.create(parent={"database_id":NOTION_DATABASE_ID}, properties=properties_to_save)
        success_count += 1
        print(f"    -> âœ… '{title}' Notion ì €ì¥ ì„±ê³µ!")
    except Exception as e:
        print(f"   ğŸš¨ [ì˜¤ë¥˜] '{title}' ì €ì¥ ì‹¤íŒ¨! ì›ì¸: {e}")

    # ì†ë„ ì œì–´ ë¡œì§ ì¶”ê°€ Gemini APIì˜ ë¶„ë‹¹ ìš”ì²­ í•œë„ 15RPMì„ ì¤€ìˆ˜í•˜ê¸° ìœ„í•´ ëŒ€ê¸°
    # ë§ˆì§€ë§‰ í•­ëª©ì—ì„œëŠ” ëŒ€ê¸°í•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ ì¡°ê±´ì¶”ê°€
    if i < len(full_new_jobs) - 1:
        print(" -> API ì†ë„ ì œì–´ë¥¼ ìœ„í•´ 5ì´ˆ ëŒ€ê¸°í•©ë‹ˆë‹¤...")
        time.sleep(5)


# ìµœì¢… ê²°ê³¼ ìš”ì•½
print("-" * 30)
print("âœ… ëª¨ë“  íŒŒì´í”„ë¼ì¸ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
print(f"  - ì´ {success_count}ê°œì˜ ìƒˆë¡œìš´ ê³µê³ ë¥¼ Notionì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
print(f"  - {failed_count}ê°œì˜ ê³µê³ ëŠ” ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
print(f"  - {filtered_count}ê°œì˜ ê³µê³ ëŠ” í•„í„°ë§ë˜ì–´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("-" * 30)