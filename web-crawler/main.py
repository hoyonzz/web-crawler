import os
import json
from datetime import datetime
from dotenv import load_dotenv
import notion_client


from crawlers.wanted_crawler import WantedCrawler
from crawlers.jobkorea_crawler import JobKoreaCrawler
from crawlers.saramin_crawler import SaraminCrawler

from data_processor.personalized_job_filter import PersonalizedJobFilter

from analysis.gemini_analyzer import analyze_job_posting


load_dotenv()

# 1. Notion API ì„¤ì •
NOTION_API_KEY = os.environ.get("NOTION_API_KEY")
NOTION_DATABASE_ID = os.environ.get("NOTION_DATABASE_ID")
notion = notion_client.Client(auth=NOTION_API_KEY)

# 2. ë°í‹°ì–´ ìˆ˜ì§‘ ë‹¨ê³„(í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸)
print("ğŸš€ ì±„ìš© ê³µê³  ìˆ˜ì§‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
all_jobs = []

# í´ë˜ìŠ¤ ìì²´ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ë‹´ì•„, í•„ìš”í•  ë•Œ ê°ì²´ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³€ê²½
crawlers_to_run = [WantedCrawler, JobKoreaCrawler, SaraminCrawler]

for crawlerClass in crawlers_to_run:
    crawler = crawlerClass()
    crawler_name = type(crawler).__name__
    print(f"--- {crawler_name} í¬ë¡¤ë§ ì‹œì‘ ---")
    
    try:
        # 1ë‹¨ê³„: ëª©ë¡ í˜ì´ì§€ì—ì„œ ì‹ ì… ê³µê³ ë§Œ í•„í„°ë§í•˜ì—¬ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        crawled_jobs_list = crawler.crawl(keyword='ë°±ì—”ë“œ', pages_to_crawl=1, is_newbie=True)
        
        # 2ë‹¨ê³„: ê° ê³µê³ ì˜ ìƒì„¸ í˜ì´ì§€ì— ë°©ë¬¸í•˜ì—¬ ë³¸ë¬¸ ìˆ˜ì§‘ - í…ŒìŠ¤íŠ¸ ë²”ìœ„ë¥¼ 3ê°œë¡œ ì œí•œ
        jobs_to_process = crawled_jobs_list[:3]
        print(f"   -> {len(crawled_jobs_list)}ê°œ ì¤‘ {len(jobs_to_process)}ê°œë§Œ ìƒì„¸ ë¶„ì„ ì§„í–‰...")
        
        for i, job in enumerate(jobs_to_process):
            job_link = job['link']
            print(f"   ({i+1}/{len(jobs_to_process)}) ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {job_link[:50]}...") # ë§í¬ ì•ë¶€ë¶„ë§Œ ì¶œë ¥

            details = crawler.get_job_description(job_link)
            job['description'] = details.get('description', '')
            job['deadline'] = details.get('deadline', 'ìƒì‹œì±„ìš©')

        all_jobs.extend(jobs_to_process)
        print(f"--- {crawler_name} í¬ë¡¤ë§ ì™„ë£Œ")
    
    except Exception as e:
        print(f" [ì „ì²´ í¬ë¡¤ëŸ¬ ì˜¤ë¥˜] {crawler_name} ì‹¤í–‰ ì¤‘ ë¬¸ì œ ë°œìƒ:{e}")

    finally:
        crawler.close_driver()

# 3. ë°ì´í„° ì²˜ë¦¬ ë° ì €ì¥ ë‹¨ê³„ (í•„í„°ë§ -> ë¶„ì„ -> ì €ì¥ íŒŒì´í”„ë¼ì¸)
print(f"\nâœ… ì´ {len(all_jobs)}ê°œì˜ ì±„ìš© ê³µê³  ìˆ˜ì§‘ ì™„ë£Œ. Notion DBì™€ ë¹„êµ ë° ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

collection_date = datetime.now().strftime("%Y-%m-%d")
success_count = 0
duplicate_count = 0
job_filter = PersonalizedJobFilter()
filtered_count = 0

# ì¿¼ë¦¬ ì¡°íšŒ ê¸°ëŠ¥
for i, job in enumerate(all_jobs):
    title = job.get('title', 'ì œëª© ì—†ìŒ')
    link = job.get('link')
    company = job.get('company', 'íšŒì‚¬ëª… ì—†ìŒ')
    source = job.get('source', 'ì¶œì²˜ ì—†ìŒ')
    description = job.get('description', '')
    deadline = job.get('deadline', 'ìƒì‹œì±„ìš©')

    # 1. ì¤‘ë³µí™•ì¸
    try:
        response = notion.databases.query(
            database_id=NOTION_DATABASE_ID,
            filter = {"property": "ë§í¬", "url": {"equals": link}}
        )

        # 2. ì¿¼ë¦¬ ê²°ê³¼ì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        if len(response['results']) > 0:
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë°ì´í„°
            print(f" [{i+1}/{len(all_jobs)}] [ì¤‘ë³µ] {title}")
            duplicate_count += 1
            continue

    except Exception as e:
        print(f" [ì˜¤ë¥˜ ë°œìƒ] Notion DB ì¡°íšŒ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}")
        continue

    # 2ë‹¨ê³„: ê°œì¸í™” í•„í„°ë§
    is_relevant, score = job_filter.calculate_relevance_score(title, description)
    if not is_relevant:
        print(f"   [{i+1}/{len(all_jobs)}] [í•„í„°ë§ë¨] {title} (ì ìˆ˜: {score})")
        continue
    print(f"  [{i+1}/{len(all_jobs)}] [ì‹ ê·œ/í†µê³¼] '{title}' (ì ìˆ˜: {score}) -> Gemini ë¶„ì„ ì‹œì‘...")


    # 3ë‹¨ê³„: Gemini AI ë¶„ì„
    analysis_result = None
    
    if description:
        analysis_result = analyze_job_posting(description)

    # 4ë‹¨ê³„: Notionì— ì €ì¥
    # properties ë”•ì…”ë„ˆë¦¬ë¥¼ ë™ì ìœ¼ë¡œ êµ¬ì„±
    properties_to_save = {
        'ì§ë¬´': {
            'title': [{'text': {'content': title}}]
            },
        'íšŒì‚¬ëª…': {
            'rich_text': [{'text': {'content': company}}]
            },
        'ë§í¬': {
            'url': link
            },
        'ì¶œì²˜':{'rich_text': [{'text': {'content': source }}]
            },
        'ìˆ˜ì§‘ì¼': {
            'date': {'start': collection_date}
        },
    }

    # Gemini ë¶„ì„ ê²°ê³¼ê°€ ìˆì„ ê²½ìš°, í•´ë‹¹ ì†ì„±ë“¤ì„ ì¶”ê°€
    if analysis_result:
        summary = analysis_result.get('summary', '')
        skills = analysis_result.get('required_skills', [])
        properties_to_save['AI ìš”ì•½'] = {'rich_text': [{'text': {'content': summary}}]}
        if skills:
            properties_to_save['í•µì‹¬ ê¸°ìˆ '] = {'multi_select': [{'name': skill} for skill in skills[:100]]}
    
    try:
        notion.pages.create(
            parent={"database_id": NOTION_DATABASE_ID},
            properties=properties_to_save
        )
        success_count += 1

    except Exception as e:
        print(f" ğŸš¨ [ì˜¤ë¥˜] '{title}' ì €ì¥ ì‹¤íŒ¨! ì›ì¸: {e}")


# ìµœì¢… ê²°ê³¼ ìš”ì•½
print("-" * 30)
print(f"  - ì´ {success_count}ê°œì˜ ìƒˆë¡œìš´ ê³µê³ ë¥¼ Notionì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
print(f"  - {duplicate_count}ê°œì˜ ê³µê³ ëŠ” ì´ë¯¸ ì¡´ì¬í•˜ì—¬ ê±´ë„ˆë›°ì—ˆìŠµë‹ˆë‹¤.")
print(f"  - {filtered_count}ê°œì˜ ê³µê³ ëŠ” í•„í„°ë§ë˜ì–´ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
print("-" * 30)