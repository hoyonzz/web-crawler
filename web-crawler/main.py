import os
import json
from datetime import datetime
from dotenv import load_dotenv
import notion_client


from crawlers.wanted_crawler import WantedCrawler
from crawlers.jobkorea_crawler import JobKoreaCrawler
from crawlers.saramin_crawler import SaraminCrawler

from data_processor.personalized_job_filter import PersonalizedJobFilter
from analysis.gemini_analyzer import analyze_job_posting_with_ai


load_dotenv()

# ν…μ¤νΈ μ‹ λ¶„μ„ν•  μµλ€ κ°―μ
TEST_MODE_LIMIT = 3

# μ„¤μ • μ΄κΈ°ν™”
NOTION_API_KEY = os.environ.get("NOTION_API_KEY")
NOTION_DATABASE_ID = os.environ.get("NOTION_DATABASE_ID")
notion = notion_client.Client(auth=NOTION_API_KEY)
job_filter = PersonalizedJobFilter()

# νμ΄ν”„λΌμΈ μ‹μ‘ 
print("π€ μ±„μ© κ³µκ³  μμ§‘μ„ μ‹μ‘ν•©λ‹λ‹¤.")
all_new_jobs = []
# ν΄λμ¤ μμ²΄λ¥Ό λ¦¬μ¤νΈμ— λ‹΄μ•„, ν•„μ”ν•  λ• κ°μ²΄λ¥Ό μƒμ„±ν•λ” λ°©μ‹μΌλ΅ λ³€κ²½
crawlers_to_run = [WantedCrawler, JobKoreaCrawler, SaraminCrawler]

for crawlerClass in crawlers_to_run:
    crawler = crawlerClass()
    crawler_name = type(crawler).__name__
    print(f"\n--- {crawler_name} ν¬λ΅¤λ§ μ‹μ‘ ---")
    
    try:
        jobs = crawler.crawl(keyword='λ°±μ—”λ“', pages_to_crawl=1, is_newbie=True)
        new_count = 0
        
        # 3. 2λ‹¨κ³„: Notion DBμ™€ λΉ„κµν•μ—¬ μ΄λ―Έ μμ§‘λ κ³µκ³  μ μ™Έ
        for i, job in enumerate(jobs):
            link = job.get('link')
            if not link: continue

            print(f" β³ ({i+1}/{len(jobs)}) DB μ¤‘λ³µ ν™•μΈ μ¤‘: {job.get('title', '')[:30]}...")
            response = notion.databases.query(
                database_id=NOTION_DATABASE_ID,
                filter={"property": "λ§ν¬", "url": {"equals": link}}
            )
            if len(response['results']) == 0:
                all_new_jobs.append(job)
                new_count += 1
        print(f"   -> {new_count}κ°μ μƒλ΅μ΄ κ³µκ³ λ¥Ό λ°κ²¬ν–μµλ‹λ‹¤.")
    except Exception as e:
        print(f" [μ¤λ¥] {crawler_name} μ‹¤ν–‰ μ¤‘ λ¬Έμ  λ°μƒ:{e}")

    finally:
        crawler.close_driver()

print(f"\nβ…[1λ‹¨κ³„ μ™„λ£] μ΄ {len(all_new_jobs)}κ°μ μƒλ΅μ΄ κ³µκ³ λ¥Ό λ°κ²¬ν–μµλ‹λ‹¤.")

if TEST_MODE_LIMIT is not None and len(all_new_jobs) > TEST_MODE_LIMIT:
    print(f"\nβ οΈ μ‹μ΄μ „ λ¨λ“: {len(all_new_jobs)}κ°μ μ‹ κ· κ³µκ³  μ¤‘ {TEST_MODE_LIMIT}κ°λ§ μƒμ„Έ λ¶„μ„μ„ μ§„ν–‰ν•©λ‹λ‹¤.")
    jobs_to_process_details = all_new_jobs[:TEST_MODE_LIMIT]
else:
    jobs_to_process_details = all_new_jobs

print(f"\n[2λ‹¨κ³„] {len(jobs_to_process_details)}κ° κ³µκ³ μ μƒμ„Έ μ •λ³΄ μμ§‘ μ‹μ‘...")
full_new_jobs, failed_details_count = [], 0
crawler_instances = {cls.__name__: cls() for cls in [WantedCrawler, JobKoreaCrawler, SaraminCrawler]}

for i, job in enumerate(jobs_to_process_details):
    source_crawer_name = job.get('source', '') + "Crawler"
    crawler = crawler_instances.get(source_crawer_name)
    link = job.get('link')

    if not crawler or not link:
        failed_details_count += 1
        continue

    print(f"\n[{i+1}/{len(jobs_to_process_details)}] μƒμ„Έ μ •λ³΄ μμ§‘: {job.get('title', '')[:30]}...")
    try:
        details = crawler.get_job_description(link)
        job['description'] = details.get('description', '')
        job['deadline'] = details.get('deadline', 'ν™•μΈ ν•„μ”')
        full_new_jobs.append(job)
    except Exception as e:
        print(f" [μ¤λ¥] μƒμ„Έ μ •λ³΄ μμ§‘ μ¤‘ λ¬Έμ  λ°μƒ: {e}")
        failed_details_count += 1

for crawler in crawler_instances.values():
    crawler.close_driver()
print(f"\nβ…[2λ‹¨κ³„ μ™„λ£] {len(full_new_jobs)}κ°μ κ³µκ³ μ— λ€ν• μƒμ„Έ μ •λ³΄ μμ§‘μ„ μ™„λ£ν–μµλ‹λ‹¤.")


# 3λ‹¨κ³„: λ¶„μ„ λ° Notion μ €μ¥
print("\n[3λ‹¨κ³„] κ°μΈν™” ν•„ν„°λ§, AI λ¶„μ„ λ° Notion μ €μ¥ μ‹μ‘...")
success_count = 0
filtered_count = 0

for i, job in enumerate(full_new_jobs):
    title = job.get('title', 'μ λ© μ—†μ')
    description = job.get('description', '')

    print(f"\n--- ({i+1} / {len(full_new_jobs)}) λ¶„μ„ μ¤‘: {title}---")

    # 3-1. κ°μΈν™” ν•„ν„°λ§
    is_relevant, score = job_filter.calculate_relevance_score(title, description)
    if not is_relevant:
        print(f" -> [ν•„ν„°λ§λ¨] κ΄€λ ¨λ„κ°€ λ‚®μ•„ κ±΄λ„λλ‹λ‹¤. (μ μ: {score})")
        filtered_count += 1
        continue

    print(f" -> [ν†µκ³Ό] κ΄€λ ¨λ„ λ†’μ (μ μ: {score}). AI λ¶„μ„μ„ μ‹μ‘ν•©λ‹λ‹¤...")

    # 3-2. Gemini AI λ¶„μ„
    matched_skills = job_filter.extract_matched_skills(description)
    ai_analysis_json = analyze_job_posting_with_ai(title, description, matched_skills)

    if not ai_analysis_json:
        print(" -> π¨[μ¤λ¥] AI λ¶„μ„μ— μ‹¤ν¨ν–μµλ‹λ‹¤. λ‹¤μ κ³µκ³ λ΅ λ„μ–΄κ°‘λ‹λ‹¤.")
        continue

    # 3-3. Notionμ— μ €μ¥
    ai_analysis_text = json.dumps(ai_analysis_json, ensure_ascii=False, indent = 2)
    properties_to_save = {
        'μ§λ¬΄':{'title':[{'text':{'content':title}}]},
        'νμ‚¬λ…':{'rich_text': [{'text':{'content':job.get('company', 'νμ‚¬λ… μ—†μ')}}]},
        'λ§ν¬':{'url':job.get('link')},
        'μ¶μ²':{'rich_text':[{'text':{'content':job.get('source', 'μ¶μ² μ—†μ')}}]},
        'μμ§‘μΌ':{'date':{'start':datetime.now().strftime("%Y-%m-%d")}},
        'λ§κ°μΌ':{'rich_text':[{'text':{'content':job.get('deadline', 'ν™•μΈ ν•„μ”')}}]},
        'AI λ¶„μ„ κ²°κ³Ό': {'rich_text':[{'text':{'content':ai_analysis_text}}]}
    }

    try:
        notion.pages.create(parent={"database_id":NOTION_DATABASE_ID}, properties=properties_to_save)
        success_count += 1
        print(f"    -> β… '{title}' Notion μ €μ¥ μ„±κ³µ!")
    except Exception as e:
        print(f"   π¨ [μ¤λ¥] '{title}' μ €μ¥ μ‹¤ν¨! μ›μΈ: {e}")


# μµμΆ… κ²°κ³Ό μ”μ•½
print("-" * 30)
print("β… λ¨λ“  νμ΄ν”„λΌμΈμ΄ μ™„λ£λμ—μµλ‹λ‹¤.")
print(f"  - μ΄ {success_count}κ°μ μƒλ΅μ΄ κ³µκ³ λ¥Ό Notionμ— μ €μ¥ν–μµλ‹λ‹¤.")
print(f"  - {len(all_new_jobs) - len(full_new_jobs)}κ°μ κ³µκ³ λ” μƒμ„Έ μ •λ³΄ μμ§‘ μ¤‘ μ¤λ¥κ°€ λ°μƒν–μµλ‹λ‹¤.")
print(f"  - {filtered_count}κ°μ κ³µκ³ λ” ν•„ν„°λ§λμ–΄ μ μ™Έλμ—μµλ‹λ‹¤.")
print("-" * 30)