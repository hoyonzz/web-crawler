# crawler.yml

name: Daily Job Crawler

on:
  schedule:
    # 매일 UTC 기준 20시에 실행 (한국 시간 새벽 5시)
    - cron: '0 20 * * *'
  
  
  # 수동 실행 버튼
  workflow_dispatch:

  # main 브랜치에 push 시 실행
  push:
    branches: [ main ]

jobs:
  build:
    runs-on: ubuntu-latest
    
    # 이 job의 모든 run 명령어는 web-crowler 폴더 안에서 실행됩니다.
    defaults:
      run:
        working-directory: ./web-crawler

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
      
      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Run crawler
        run: python main.py
        env:
          NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}
          NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
      
      - name: Upload Artifacts on failure # 실패시 결과물 업로드
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: error-artifacts
          path: |
            web-crawler/error_screenshot.png
            web-crawler/error_page.html